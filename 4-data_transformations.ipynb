{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99e6b75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66aff7c1",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a82a16",
   "metadata": {},
   "source": [
    "### Spark session initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4922786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 15:58:08 WARN Utils: Your hostname, DESKTOP-PFNTFJ1 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/08/28 15:58:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/08/28 15:58:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.port: 45409\n",
      "spark.rdd.compress: True\n",
      "spark.app.id: local-1756396693342\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.master: local[*]\n",
      "spark.submit.pyFiles: \n",
      "spark.app.startTime: 1756396691943\n",
      "spark.executor.id: driver\n",
      "spark.submit.deployMode: client\n",
      "spark.app.name: PySpark Data Transformations\n",
      "spark.driver.host: 10.255.255.254\n",
      "spark.ui.showConsoleProgress: true\n",
      "Spark Job URL: http://10.255.255.254:4040\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(spark_home=\"/home/prabhakar/mybin/spark-3.0.2-bin-hadoop2.7-hive1.2\")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark Data Transformations\").getOrCreate()\n",
    "\n",
    "configs = spark.sparkContext.getConf().getAll()\n",
    "for key, value in configs:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get the Spark UI URL\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "\n",
    "print(f\"Spark Job URL: {spark_ui_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b118a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5e23820",
   "metadata": {},
   "source": [
    "## UDF - User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9894a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------+\n",
      "| id|      name|capitalized_name|\n",
      "+---+----------+----------------+\n",
      "|  1|  john doe|        John Doe|\n",
      "|  2|jane smith|      Jane Smith|\n",
      "+---+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"1\", \"john doe\"), (\"2\", \"jane smith\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Define a UDF to capitalize names\n",
    "# This is a python code and not a pyspark code\n",
    "\n",
    "# Drawbacks:\n",
    "# UDF wil be called for each record in the dataframe\n",
    "# UDF is a blackbox for pyspark and may ont be optimised well.\n",
    "def capitalize_name(name):\n",
    "   return name.title()\n",
    "\n",
    "# Registering the udf\n",
    "# Higher Order Functions ==> Functions which take another function as arguments\n",
    "capitalize_udf = udf(capitalize_name, StringType())\n",
    "# capitalize_udf = udf(lambda: title(name))\n",
    "\n",
    "# Apply UDF\n",
    "df.withColumn(\"capitalized_name\", capitalize_udf(df[\"name\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efeaf0",
   "metadata": {},
   "source": [
    "Why UDFs Are Inefficient\n",
    "\n",
    "Lack of Optimization: PySpark UDFs are treated as black boxes by Spark's Catalyst optimizer. This prevents Spark from applying query optimizations like predicate pushdown or column pruning.\n",
    "\n",
    "Serialization Overhead: When using Python UDFs, data must be serialized and transferred between the JVM (Spark engine) and Python runtime. This process involves significant overhead.\n",
    "\n",
    "Row-by-Row Execution: UDFs process data row by row, which is slower compared to vectorized operations provided by Spark's built-in functions.\n",
    "\n",
    "Null Handling Issues: UDFs require explicit null handling. Failing to do so can lead to runtime errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b87fb7",
   "metadata": {},
   "source": [
    "### String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e12c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|      name|upper_name|\n",
      "+---+----------+----------+\n",
      "|  1|  john doe|  JOHN DOE|\n",
      "|  2|jane smith|JANE SMITH|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+----------+\n",
      "| id|      name|upper_name|\n",
      "+---+----------+----------+\n",
      "|  1|  john doe|  JOHN DOE|\n",
      "|  2|jane smith|JANE SMITH|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+----------+----------+\n",
      "| id|      name|upper_name|lower_name|\n",
      "+---+----------+----------+----------+\n",
      "|  1|  john doe|  JOHN DOE|  john doe|\n",
      "|  2|jane smith|JANE SMITH|jane smith|\n",
      "+---+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper, col\n",
    "df.withColumn(\"upper_name\", upper(col(\"name\"))).show()\n",
    "df.withColumn(\"upper_name\", upper(df[\"name\"])).show()\n",
    "\n",
    "\n",
    "df.withColumn(\"upper_name\", upper(df[\"name\"])).withColumn(\"lower_name\", lower(col(\"upper_name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd9f87",
   "metadata": {},
   "source": [
    "### Date Time Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12d1c565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|      name| curr_date|\n",
      "+---+----------+----------+\n",
      "|  1|  john doe|2025-08-27|\n",
      "|  2|jane smith|2025-08-27|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+----------+\n",
      "| id|      name|epoch_time|\n",
      "+---+----------+----------+\n",
      "|  1|  john doe|1756314739|\n",
      "|  2|jane smith|1756314739|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+----------+-------------------+----------+\n",
      "| id|      name|epoch_time|     curr_timestamp| curr_date|\n",
      "+---+----------+----------+-------------------+----------+\n",
      "|  1|  john doe|1756314739|2025-08-27 17:12:19|2025-08-27|\n",
      "|  2|jane smith|1756314739|2025-08-27 17:12:19|2025-08-27|\n",
      "+---+----------+----------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, unix_timestamp, to_date, from_unixtime\n",
    "from pyspark.sql.types import IntegerType\n",
    "df.withColumn(\"curr_date\", current_date()).show()\n",
    "\n",
    "df.withColumn(\"epoch_time\", unix_timestamp()).show()\n",
    "\n",
    "df.withColumn(\"epoch_time\", unix_timestamp()). \\\n",
    "withColumn(\"curr_timestamp\", from_unixtime(col(\"epoch_time\"))). \\\n",
    "withColumn(\"curr_date\", to_date(col(\"curr_timestamp\"))). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c9c53",
   "metadata": {},
   "source": [
    "### Numeric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4565787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          SQRT(id)|\n",
      "+------------------+\n",
      "|               1.0|\n",
      "|1.4142135623730951|\n",
      "+------------------+\n",
      "\n",
      "+---+------------------+\n",
      "| id|        sqrt_value|\n",
      "+---+------------------+\n",
      "|  1|               1.0|\n",
      "|  2|1.4142135623730951|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import abs, sqrt\n",
    "df.select(sqrt(col(\"id\"))).show()\n",
    "df.withColumn(\"sqrt_value\", sqrt(col(\"id\"))).select(\"id\", \"sqrt_value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6467952",
   "metadata": {},
   "source": [
    "### Conditional Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "393a867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------+\n",
      "| id|      name|capitalize_name|\n",
      "+---+----------+---------------+\n",
      "|  1|  john doe|       john doe|\n",
      "|  2|jane smith|     JANE SMITH|\n",
      "+---+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"capitalize_name\", when(col(\"id\") > 1, upper(col(\"name\"))).otherwise(col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4d521",
   "metadata": {},
   "source": [
    "### Type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e678fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- id_number_type: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "df.printSchema()\n",
    "\n",
    "# convert id String type to number type\n",
    "df.withColumn(\"id_number_type\", col(\"id\").cast(IntegerType())).printSchema()\n",
    "df2 = df.select(col(\"id\").cast(IntegerType()), col(\"name\"))\n",
    "\n",
    "df.printSchema()\n",
    "df2.printSchema()\n",
    "\n",
    "\n",
    "df. \\\n",
    "withColumn(\"id_number_type\", col(\"id\").cast(IntegerType())). \\\n",
    "drop(col(\"id\")). \\\n",
    "withColumnRenamed(\"id_number_type\", \"id\"). \\\n",
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   name|language|\n",
      "+-------+--------+\n",
      "|  Alice|  Python|\n",
      "|  Alice|     SQL|\n",
      "|    Bob|    Java|\n",
      "|    Bob|   Scala|\n",
      "|Charlie|     C++|\n",
      "|Charlie|      Go|\n",
      "|Charlie|    Rust|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n",
    "\n",
    "# Sample data with array column\n",
    "data = [\n",
    "    (\"Alice\", [\"Python\", \"SQL\"]),\n",
    "    (\"Bob\", [\"Java\", \"Scala\"]),\n",
    "    (\"Charlie\", [\"C++\", \"Go\", \"Rust\"])\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"languages\"])\n",
    "\n",
    "# Apply explode to 'languages' column\n",
    "exploded_df = df.select(\"name\", explode(\"languages\").alias(\"language\"))\n",
    "\n",
    "# Show result\n",
    "exploded_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef510347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|value|\n",
      "+-----+-----+\n",
      "|    1|    2|\n",
      "|    1|    3|\n",
      "|    2|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+\n",
      "|group|values|sum_squares|\n",
      "+-----+------+-----------+\n",
      "|    1|[2, 3]|         13|\n",
      "|    2|   [4]|         16|\n",
      "+-----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(1, 2), (1, 3), (2, 4)]\n",
    "df = spark.createDataFrame(data, [\"group\", \"value\"])\n",
    "df.show()                                                                  \n",
    "from pyspark.sql.functions import udf, collect_list\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def sum_of_squares(values):\n",
    "    return sum(x * x for x in values)\n",
    "\n",
    "sum_of_squares_udf = udf(sum_of_squares, LongType())      \n",
    "df_grouped = df.groupBy(\"group\").agg(collect_list(\"value\").alias(\"values\"))\n",
    "df_result = df_grouped.withColumn(\"sum_squares\", sum_of_squares_udf(\"values\"))\n",
    "df_result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c03af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|  city|\n",
      "+---+-----+------+\n",
      "|  1|Alice|Mumbai|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+---------------+\n",
      "| id|         parsed|\n",
      "+---+---------------+\n",
      "|  1|[Alice, Mumbai]|\n",
      "+---+---------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- json_str: string (nullable = true)\n",
      " |-- parsed: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### parsed json string \n",
    "\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "\n",
    "# Sample JSON string\n",
    "data = [(\"1\", '{\"name\":\"Alice\",\"city\":\"Mumbai\"}')]\n",
    "df = spark.createDataFrame(data, [\"id\", \"json_str\"])\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON string\n",
    "parsed_df = df.withColumn(\"parsed\", from_json(\"json_str\", schema))\n",
    "parsed_df.select(\"id\", \"parsed.*\").show()\n",
    "parsed_df.select(\"id\", \"parsed\").show()\n",
    "parsed_df.select(\"id\", \"parsed.*\").printSchema()\n",
    "parsed_df.select(\"id\", \"parsed\").printSchema()\n",
    "\n",
    "parsed_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a4afd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|  city|\n",
      "+---+-----+------+\n",
      "|  1|Alice|Mumbai|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extracting JSON fields\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df.select(\n",
    "    \"id\",\n",
    "    get_json_object(\"json_str\", \"$.name\").alias(\"name\"),\n",
    "    get_json_object(\"json_str\", \"$.city\").alias(\"city\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fe9b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- c0: string (nullable = true)\n",
      " |-- c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df.select(\"id\", json_tuple(\"json_str\", \"name\", \"city\")).printSchema() #.toDF(\"id\", \"name\", \"city\").show()\n",
    "df.select(\"id\", json_tuple(\"json_str\", \"name\", \"city\")).toDF(\"id\", \"name\", \"city\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec166fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           json_back|\n",
      "+---+--------------------+\n",
      "|  1|{\"name\":\"Alice\",\"...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Converting STRUCT to JSON with to_json()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "# Assuming 'parsed' column is a struct\n",
    "jsonified_df = parsed_df.withColumn(\"json_back\", to_json(\"parsed\"))\n",
    "jsonified_df.select(\"id\", \"json_back\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
