{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ad991e",
   "metadata": {},
   "source": [
    "Here’s a practical, “what to use when” guide to **`repartition`** and **`coalesce`** in PySpark—focused on **parallelism**, **skew**, and **small files**.\n",
    "\n",
    "---\n",
    "\n",
    "# What they do (in one line)\n",
    "\n",
    "* **`repartition()`**: **wide** transformation → causes a **shuffle**. Can **increase or decrease** partitions. Can also **hash-partition by columns** for better balance/locality.\n",
    "* **`coalesce()`**: **narrow** transformation → **no shuffle**. Can only **decrease** partitions by merging existing ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Managing Parallelism (number of tasks)\n",
    "\n",
    "### When you need **more** parallelism (e.g., many cores idle, slow stages)\n",
    "\n",
    "Use **`repartition(n)`** to increase partitions and spread work across executors.\n",
    "\n",
    "```python\n",
    "# Increase from, say, 50 to 400 partitions to use the cluster better\n",
    "df = df.repartition(400)  # round-robin shuffle\n",
    "```\n",
    "\n",
    "If operations are keyed (joins, windows), prefer **column-based** repartition to keep related rows together:\n",
    "\n",
    "```python\n",
    "df = df.repartition(\"user_id\")              # hash by user_id, #parts = spark.sql.shuffle.partitions\n",
    "df = df.repartition(800, \"user_id\")         # explicit partition count + hash by column\n",
    "```\n",
    "\n",
    "### When you need **less** parallelism (too many tiny tasks)\n",
    "\n",
    "* If data is already fairly balanced, use **`coalesce(target)`** (cheaper, no shuffle):\n",
    "\n",
    "```python\n",
    "df = df.coalesce(100)  # merge adjacent partitions\n",
    "```\n",
    "\n",
    "* If data is **not** balanced (some partitions huge, others tiny), use **`repartition(target)`** to **rebalance** via a shuffle.\n",
    "\n",
    "> Tip: A common steady-state target is \\~**128–512 MB per partition** (depends on executor memory/IO).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Handling Skew (one or few partitions much larger than others)\n",
    "\n",
    "**Symptoms**: One task runs far longer; joins “hang” on the last reducer.\n",
    "\n",
    "**Tools & patterns**\n",
    "\n",
    "1. **Repartition by the skew key** to ensure even hashing:\n",
    "\n",
    "   ```python\n",
    "   big = big.repartition(2000, \"join_key\")  # more buckets to dilute a hot key\n",
    "   ```\n",
    "\n",
    "2. **Salting** the skewed key (split a hot key across multiple buckets), then desalt after join:\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql import functions as F\n",
    "\n",
    "   salt_buckets = 20\n",
    "   big_salted = big.withColumn(\"salt\", (F.rand()*salt_buckets).cast(\"int\"))\n",
    "   # replicate small side across salt buckets (explode) or broadcast it\n",
    "   small = F.broadcast(small)  # if it fits broadcast threshold\n",
    "   joined = big_salted.join(small, [\"join_key\"], \"inner\")\n",
    "   ```\n",
    "\n",
    "3. **AQE (Adaptive Query Execution)** in Spark 3+:\n",
    "\n",
    "   ```python\n",
    "   spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "   spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "   spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "   ```\n",
    "\n",
    "   AQE can automatically split skewed partitions and coalesce small ones during shuffles.\n",
    "\n",
    "4. **Broadcast** the small side of a skewed join when possible:\n",
    "\n",
    "   ```python\n",
    "   joined = big.join(F.broadcast(small), \"join_key\")\n",
    "   ```\n",
    "\n",
    "**Rule of thumb**: If skew is the bottleneck, prefer **`repartition(by columns)`** (possibly with more partitions) or **AQE**; **`coalesce()`** won’t fix skew (it only merges, doesn’t rebalance).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Reducing Small Files (object stores like S3/ADLS/GCS, HDFS)\n",
    "\n",
    "Each Spark **task writes one file** per output directory. Too many partitions ⇒ too many **small files**.\n",
    "\n",
    "### General, non-partitioned writes\n",
    "\n",
    "* If your final dataset is already balanced:\n",
    "\n",
    "  * **`coalesce(target_files)`** (fast, no shuffle) just before `.write`:\n",
    "\n",
    "    ```python\n",
    "    target_files = 200\n",
    "    df.coalesce(target_files).write.mode(\"overwrite\").parquet(path)\n",
    "    ```\n",
    "* If it’s **not** balanced (or you want more consistent file sizes):\n",
    "\n",
    "  * **`repartition(target_files)`** before write:\n",
    "\n",
    "    ```python\n",
    "    df.repartition(200).write.parquet(path)\n",
    "    ```\n",
    "\n",
    "### Partitioned writes (e.g., `partitionBy(\"date\")`)\n",
    "\n",
    "Want **one file per partition value** (or a controlled small number)?\n",
    "\n",
    "* Use **`repartition` on the same partition columns before writing**. This ensures all rows for a given partition value live in the **same Spark partition**, so **only one task** writes that directory → typically **one file per value**.\n",
    "\n",
    "  ```python\n",
    "  out = df.repartition(\"date\")  # hash by date; each date goes to exactly one partition\n",
    "  out.write.partitionBy(\"date\").parquet(path)  # usually one file per date value\n",
    "  ```\n",
    "* For multi-column partitions:\n",
    "\n",
    "  ```python\n",
    "  out = df.repartition(\"country\", \"date\")\n",
    "  out.write.partitionBy(\"country\", \"date\").parquet(path)\n",
    "  ```\n",
    "* If you need a **specific number of files per partition value** (e.g., 2 files per `date`), there isn’t a native “coalesce per key” primitive. Two approaches:\n",
    "\n",
    "  * Set **more total partitions** than distinct keys, so some keys share a partition (you’ll still get one file per key with `repartition(partCols)`), then **use `maxRecordsPerFile`** to limit file size:\n",
    "\n",
    "    ```python\n",
    "    (df.repartition(\"date\")\n",
    "       .write\n",
    "       .option(\"maxRecordsPerFile\", 5_000_000)  # coarse file size control\n",
    "       .partitionBy(\"date\")\n",
    "       .parquet(path))\n",
    "    ```\n",
    "  * Or write each partition value in a loop (only for moderate cardinalities).\n",
    "\n",
    "> Avoid `coalesce(1)` on big data—it forces a single task to write a single file (slow, memory pressure, single point of failure). It’s fine only for tiny outputs.\n",
    "\n",
    "### Read-side small files (many tiny input files)\n",
    "\n",
    "Small input files create too many splits and task overhead. Options:\n",
    "\n",
    "* Increase split size:\n",
    "\n",
    "  ```python\n",
    "  spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728)  # 128 MB\n",
    "  ```\n",
    "* Compact upstream (Delta/iceberg optimize, or periodic compaction job).\n",
    "* After reading, use `repartition()` if subsequent stages need fewer, larger tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing between `repartition` and `coalesce`: Decision guide\n",
    "\n",
    "* **Increase parallelism** → `repartition(n)`\n",
    "* **Data skew hurting performance** → `repartition(n, *cols)` (and/or AQE, salting, broadcast)\n",
    "* **Just fewer output files; data already even** → `coalesce(n)`\n",
    "* **Fewer output files; data uneven** → `repartition(n)`\n",
    "* **Partitioned write and want \\~1 file per partition value** → `repartition(partitionCols)` **then** `write.partitionBy(partitionCols)`\n",
    "* **Absolute minimal shuffle cost** → prefer `coalesce()` (only when safe)\n",
    "\n",
    "---\n",
    "\n",
    "## Common code snippets\n",
    "\n",
    "### Balance & write (non-partitioned), target \\~256MB files\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Estimate target partitions (rough heuristic if you know size)\n",
    "total_size_bytes = 1_000_000_000_000  # 1 TB\n",
    "target_file_size = 256 * 1024 * 1024  # 256 MB\n",
    "target_parts = int((total_size_bytes + target_file_size - 1) // target_file_size)\n",
    "\n",
    "balanced = df.repartition(target_parts)  # or coalesce if already balanced\n",
    "(balanced\n",
    "  .write\n",
    "  .option(\"maxRecordsPerFile\", 10_000_000)  # optional\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(path))\n",
    "```\n",
    "\n",
    "### Partitioned write with one file per partition value\n",
    "\n",
    "```python\n",
    "df.repartition(\"event_date\").write.partitionBy(\"event_date\").parquet(path)\n",
    "```\n",
    "\n",
    "### Skewed join quick fixes\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "big  = big.repartition(2000, \"user_id\")\n",
    "small = F.broadcast(small)  # if small enough\n",
    "joined = big.join(small, \"user_id\", \"inner\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Pitfalls & tips\n",
    "\n",
    "* `coalesce()` can create **few very large partitions** → risk of OOM/GC pressure on those tasks.\n",
    "* `repartition()` always shuffles (costly) but **evens out** data—use it when size/latency justify it.\n",
    "* `repartition(col)` uses **hash partitioning**: **all rows for the same key go to the same partition** (good for per-key file consolidation).\n",
    "* Don’t forget **`spark.sql.shuffle.partitions`** defaults (often 200). Set it deliberately for your workload or rely on **AQE coalescing**.\n",
    "* For very large datasets, prefer **many medium files** (e.g., 128–512 MB) over a few huge ones or thousands of tiny ones.\n",
    "\n",
    "---\n",
    "\n",
    "If you share your workload shape (cluster cores, dataset size, partition columns, and where skew appears), I can suggest exact partition counts and a drop-in snippet.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
